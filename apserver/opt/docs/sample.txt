## RAG（検索拡張生成）について
huggingfaceなどからllmをダウンロードしてそのままチャットに利用した際、参照する情報はそのllmの学習当時のものとなります。（当たり前ですが）学習していない会社の社内資料や個人用PCのローカルなテキストなどはllmの知識にありません。
このような存在しない情報をllmに与える（参照させる）手法がRAGです。
ファインチューニングという手法もありますが、そちらはllmに再学習を行わせる手法です。ファインチューニングでは、llm自体に学習させることで知識を追加しますが、RAGの場合は用意したデータベースから検索することで、追加の情報を与えることができます。
イメージ的には以下のような感じです。
・ファインチューニング: 新しい情報を勉強させる。
・RAG: 新しい情報が記載された本を持たせる。
今回は比較的手軽にできるRAGを使用します。
## RAGの手順
RAGの手順としては以下のようになっています。1,2は準備、3～5がチャットを行う際の処理手順です。
1. 読み込ませたい資料内の文字列を分割 & ベクトル形式(複数の次元を持つ数値のかたまり)に変換。
2. （変換した）資料をインデックスとして保存。
    ※ここでいう「インデックス」とは、ベクトル化された資料のデータベースのことです
3. llmへの質問文をベクトル形式に変換。
4. （変換した）質問文から、L2距離やコサイン類似度などのアルゴリズム用いてインデックスを検索します → 類似性のある文章を取得。
5. プロンプトに取得した文章を挿入。
    ※ 以下の場合はコンテキスト（検索で取得した文字列）が一つしかなくプロンプトも単純なため、回答も「天気は晴れです」などコンテキストとほぼ同じ答えが返るかと思います（本来は類似した文字列の上位３つや5つなど複数取得して、コンテキストにすることが多いです）。
以上がRAGの手順です。
ざっくり言うと資料をデータベース化して保存しておく → 質問文と関連ありそうな文章をデータベースから検索 → 質問文と検索した文章をまとめてllmに投げるという流れです
手順の中でベクトル形式への変換や検索に使うアルゴリズム（L2距離やコサイン類似度）などがありましたが、基本的にはベクトル検索用のライブラリ（FAISS、Chroma、Qdrantなど）が内部で行ってくれるため意識しなくても普通に使えるようになっています。
次は実際にベクトル検索ライブラリ（今回はFAISS）を使ってインデックスの作成を行います。上記に書いたRAGの手順では1と2に対応します。
